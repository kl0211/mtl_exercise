{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d38956-372d-49d5-9e11-83632954a066",
   "metadata": {},
   "source": [
    "# Task 2 & 4: Multi-Task Learning Expansion & Training Loop Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f733735-0b50-4d0f-9366-6860a9bbcb23",
   "metadata": {},
   "source": [
    "### I chosed to utilize Pytorch Lightning's framework, as I have prior experience with it, and I like the organization of the model architecture and training calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11071a5d-5e1a-48d4-88a7-2e6ce41c6160",
   "metadata": {},
   "source": [
    "#### For a Multitask model, I decided to add 2 Linear Layers after the transformer layer for classifying messages in the dataset as spam or not spam, as well as the sentiment of each message (positive, neutral, or negative). This is a completely functional model that trains on real data, and evaluates on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5899df81-cf7b-4c4d-bc2a-9e70a850577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning\n",
    "import nltk.sentiment\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn import metrics, model_selection, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cbc155-e5c1-4d0d-b812-ffaef235d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTask(lightning.LightningModule):\n",
    "    def __init__(self, transformer_model, spam_weights=None, sentiment_weights=None, learning_rate=1e-4):\n",
    "        \"\"\"\n",
    "        This is where the model's architecture is defined (layers, activations, etc.).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.save_hyperparameters(ignore=[\"transformer_model\"])\n",
    "\n",
    "        # Transformer backbone\n",
    "        self.transformer_model = transformer_model\n",
    "        for _, param in self.transformer_model.named_parameters():  # Freeze all transformer layers\n",
    "            param.requires_grad = False\n",
    "        hidden_size = self.transformer_model.config.hidden_size\n",
    "\n",
    "        # Task layers\n",
    "        self.spam_head = torch.nn.Linear(hidden_size, 1)\n",
    "        self.sentiment_head = torch.nn.Linear(hidden_size, 3)\n",
    "\n",
    "        # Weights. Need to use register_buffer so that the tensors follow the device of the model\n",
    "        self.register_buffer(\n",
    "            \"spam_weights\",\n",
    "            torch.tensor(spam_weights, dtype=torch.float) if spam_weights is not None else None, )\n",
    "        self.register_buffer(\n",
    "            \"sentiment_weights\",\n",
    "            torch.tensor(sentiment_weights, dtype=torch.float) if sentiment_weights is not None else None, )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Here we define how we use the modules to operate on an input batch. First, we run the inputs through the\n",
    "        transformer backbone, then we average the outputs to get a single vector for each input. Finally, we run the\n",
    "        embeddings trough each task layer. For simplicity, only 1 Linear layer is used for each task.\n",
    "        \"\"\"\n",
    "        t_outputs = self.transformer_model(**inputs)\n",
    "        embeddings = self.average_pool(t_outputs.last_hidden_state, inputs[\"attention_mask\"])\n",
    "\n",
    "        spam_logits = self.spam_head(embeddings)\n",
    "        sentiment_logits = self.sentiment_head(embeddings)\n",
    "\n",
    "        return {\n",
    "            \"spam_logits\": spam_logits,\n",
    "            \"sentiment_logits\": sentiment_logits\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        For each step, we run a batch through the forward function and compute the loss. For the spam task, we are only\n",
    "        predicting 0 (for not spam) or 1 (for spam). For the sentiment task, we are predicting one of three classes:\n",
    "        (0 for negative, 1 for neutral, 2 for positive).\n",
    "        \"\"\"\n",
    "        outputs = self(batch[\"inputs\"])\n",
    "\n",
    "        loss_spam = torch.nn.BCEWithLogitsLoss(pos_weight=self.spam_weights)(\n",
    "            outputs[\"spam_logits\"].view(-1), batch[\"spam_label\"])\n",
    "        loss_sentiment = torch.nn.CrossEntropyLoss(weight=self.sentiment_weights)(\n",
    "            outputs[\"sentiment_logits\"], batch[\"sentiment_label\"])\n",
    "        loss = loss_spam + loss_sentiment\n",
    "\n",
    "        self.log(\"train_loss_task1\", loss_spam)\n",
    "        self.log(\"train_loss_task2\", loss_sentiment)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"inputs\"])\n",
    "\n",
    "        loss_spam = torch.nn.BCEWithLogitsLoss()(\n",
    "            outputs[\"spam_logits\"].view(-1), batch[\"spam_label\"])\n",
    "        loss_sentiment = torch.nn.CrossEntropyLoss()(\n",
    "            outputs[\"sentiment_logits\"], batch[\"sentiment_label\"])\n",
    "        loss = loss_spam + loss_sentiment\n",
    "\n",
    "        self.log(\"val_loss_task1\", loss_spam, prog_bar=True)\n",
    "        self.log(\"val_loss_task2\", loss_sentiment, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def average_pool(last_hidden_states, attention_mask):\n",
    "        # We don't want to include padded tokens, so use masked_fill to zero them out.\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "class MultiTaskDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_, tokenizer_):\n",
    "        self.texts = df_[\"message\"].tolist()\n",
    "        self.spam_labels = df_[\"spam_label\"].tolist()\n",
    "        self.sentiment_labels = df_[\"sentiment_label\"].tolist()\n",
    "        self.tokenizer = tokenizer_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        spam_label = self.spam_labels[idx]\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(text, max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "        return {\n",
    "            \"inputs\": encoding,\n",
    "            \"spam_label\": torch.tensor(spam_label),\n",
    "            \"sentiment_label\": torch.tensor(sentiment_label)\n",
    "        }\n",
    "\n",
    "\n",
    "def create_dataloader(df_, tokenizer_, batch_size=32, shuffle=False):\n",
    "    dataset = MultiTaskDataset(df_, tokenizer_)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da47e46d-db72-48aa-9683-77a2666eea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/karl/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/karl/Projects/mtl_exercise/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name              | Type      | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | transformer_model | BertModel | 109 M  | eval \n",
      "1 | spam_head         | Linear    | 769    | train\n",
      "2 | sentiment_head    | Linear    | 2.3 K  | train\n",
      "--------------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "109 M     Total params\n",
      "437.941   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8453ecd439948f7abaa3444c71cbed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f0271544224d299b01a0e103456640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1700a8d7eee8440688dac5f28ebdc8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7817eaf4284649639a669a6560d40bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7168ae2b63c044818182503666d79746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1236522dd2144a9798ad2c393a0f98c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a5ebcb6bd046038876dd3d698bdfe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished. Evaluating on testing set...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"thenlper/gte-base\")\n",
    "model = transformers.AutoModel.from_pretrained(\"thenlper/gte-base\")\n",
    "df = pd.read_csv(\"hf://datasets/codesignal/sms-spam-collection/sms-spam-collection.csv\")\n",
    "df = df.rename(columns={\"label\": \"spam_label\"})\n",
    "# Change spam label to a numerical value\n",
    "df[\"spam_label\"] = df[\"spam_label\"].apply(lambda x: 1.0 if x == \"spam\" else 0.0)\n",
    "\n",
    "# Download vader_lexicon (if not already present) for marking messages with sentiment value\n",
    "nltk.download('vader_lexicon')\n",
    "sia = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n",
    "df[\"sentiment_label\"] = df[\"message\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
    "df[\"sentiment_label\"] = df[\"sentiment_label\"].apply(lambda x: 2 if x >= 0.05 else 0 if x <= -0.05 else 1)\n",
    "\n",
    "# Split dataset into 70% training, 15% validation, and 15% testing\n",
    "train_df, temp_df = model_selection.train_test_split(df, test_size=0.3,\n",
    "                                                     random_state=0)  # random_state for reproducibility\n",
    "val_df, test_df = model_selection.train_test_split(temp_df, test_size=0.5, random_state=0)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = create_dataloader(train_df, tokenizer, batch_size=32, shuffle=True)\n",
    "val_loader = create_dataloader(val_df, tokenizer, batch_size=32, shuffle=False)\n",
    "test_loader = create_dataloader(test_df, tokenizer, batch_size=32, shuffle=False)\n",
    "\n",
    "# Optional step of class weights to deal with imbalance in dataset\n",
    "spam_class_weights = utils.class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=train_df[\"spam_label\"].unique(), y=train_df[\"spam_label\"].tolist())[1:]\n",
    "sentiment_class_weights = utils.class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=train_df[\"sentiment_label\"].unique(), y=train_df[\"sentiment_label\"].tolist())\n",
    "\n",
    "# Instantiate a Trainer, and put it on GPU if available\n",
    "trainer = lightning.Trainer(max_epochs=5, accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\", devices=1)\n",
    "model = MultiTask(transformer_model=model,\n",
    "                  # spam_weights=spam_class_weights,\n",
    "                  # sentiment_weights=sentiment_class_weights\n",
    "                  )\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "print(\"Training finished. Evaluating on testing set...\")\n",
    "# Put the model into evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# And run through the test dataset and print evaluation metrics\n",
    "all_spam_preds = []\n",
    "all_spam_labels = []\n",
    "all_sentiment_preds = []\n",
    "all_sentiment_labels = []\n",
    "\n",
    "# Disable gradient updates for evaluation\n",
    "with torch.no_grad():\n",
    "    for b in test_loader:\n",
    "        batch_inputs = b[\"inputs\"]\n",
    "\n",
    "        spam_labels = b[\"spam_label\"]\n",
    "        sentiment_labels = b[\"sentiment_label\"]\n",
    "\n",
    "        batch_outputs = model(batch_inputs)\n",
    "\n",
    "        batch_spam_logits = batch_outputs[\"spam_logits\"]\n",
    "        batch_sentiment_logits = batch_outputs[\"sentiment_logits\"]\n",
    "\n",
    "        # Get predicted class indices\n",
    "        # for spam_preds, If logit is >0.5, predict it as spam\n",
    "        spam_preds = (torch.sigmoid(batch_spam_logits) > 0.5).cpu().numpy()\n",
    "        # For sentiment_preds, predict the class wit the largest value\n",
    "        sentiment_preds = torch.argmax(batch_sentiment_logits, dim=1).cpu().numpy()\n",
    "\n",
    "        all_spam_preds.extend(spam_preds)\n",
    "        all_spam_labels.extend(spam_labels.cpu().numpy())\n",
    "        all_sentiment_preds.extend(sentiment_preds)\n",
    "        all_sentiment_labels.extend(sentiment_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60457100-2490-4938-9b7e-530ac8e517e6",
   "metadata": {},
   "source": [
    "#### For metrics, precision, recall, and f-1 scores are generally a pretty good starting point, especially for tasks that have very few possible outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7651a1b2-f3b9-4021-b187-ce4f615341c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      1.00      0.94       726\n",
      "         1.0       1.00      0.15      0.27       110\n",
      "\n",
      "    accuracy                           0.89       836\n",
      "   macro avg       0.94      0.58      0.60       836\n",
      "weighted avg       0.90      0.89      0.85       836\n",
      "\n",
      "Accuracy: 0.888755980861244\n",
      "Sentiment Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.13      0.22       153\n",
      "           1       0.58      0.46      0.51       286\n",
      "           2       0.57      0.85      0.68       397\n",
      "\n",
      "    accuracy                           0.58       836\n",
      "   macro avg       0.65      0.48      0.47       836\n",
      "weighted avg       0.62      0.58      0.54       836\n",
      "\n",
      "Accuracy: 0.5825358851674641\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics\n",
    "print(\"Spam Classification Report:\")\n",
    "print(metrics.classification_report(all_spam_labels, all_spam_preds))\n",
    "print(\"Accuracy:\", metrics.accuracy_score(all_spam_labels, all_spam_preds))\n",
    "\n",
    "print(\"Sentiment Classification Report:\")\n",
    "print(metrics.classification_report(all_sentiment_labels, all_sentiment_preds))\n",
    "print(\"Accuracy:\", metrics.accuracy_score(all_sentiment_labels, all_sentiment_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be74df92-1daa-41d9-8a21-6815698363dd",
   "metadata": {},
   "source": [
    "#### If I had more time, I would look deeper into handling the class imbalance and get more creative. Some thoughts are looking at alternative methods for labeling the sentiments. Another is trying different class weight calcuations (maybe doing a manual weighting). Or, writing a custom loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7ce90-5610-4a09-bfaa-79f7847027f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b218ed-4b40-44fe-ae99-b55ce9f53469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
